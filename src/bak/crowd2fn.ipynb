{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_dir = '/disk_4/KFN/crowdsourcing/'\n",
    "M = ['M1', 'M2', 'M3', 'M4', 'M5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    alldata, data = [],[]\n",
    "    for m in M:\n",
    "        fname = crowd_dir+m+'_data.json'\n",
    "        with open(fname, 'r') as f:\n",
    "            d = json.load(f)\n",
    "            alldata += d\n",
    "    data = {}\n",
    "    for d in alldata:\n",
    "        if 'prop' in d['id'] or 'sejong' in d['id']:\n",
    "            tokens = d['tokens']\n",
    "            lu = d['lu']\n",
    "            lus = ['_' for i in range(len(tokens))]\n",
    "            args = ['_' for i in range(len(tokens))]\n",
    "            for tid in d['target']:\n",
    "                lus[tid] = lu\n",
    "            for arg in d['arguments']:\n",
    "                arg_id = arg['arg_id']\n",
    "                n = 0\n",
    "                for tok_id in arg['tokens']:\n",
    "                    args[tok_id] = str(arg_id)+'-'+str(n)\n",
    "                    n += 1\n",
    "            sent = []\n",
    "            sent.append(tokens)\n",
    "            sent.append(lus)\n",
    "            sent.append(args)\n",
    "            data[d['sent_id']] = sent\n",
    "    return data\n",
    "    \n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "crowdsourcing_data = []\n",
    "for m in M:\n",
    "    fname = crowd_dir+m+'_data.json'\n",
    "    with open(fname, 'r') as f:\n",
    "        m_data = json.load(f)\n",
    "        crowdsourcing_data += m_data\n",
    "print(len(crowdsourcing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propbank'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sid2source(sid):    \n",
    "    for d in crowdsourcing_data:\n",
    "        if sid == d['sent_id']:\n",
    "            source = d['source']\n",
    "            break\n",
    "    return source\n",
    "sid2source(10062)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1\n",
      "M2\n",
      "M3\n",
      "M4\n",
      "M5\n"
     ]
    }
   ],
   "source": [
    "def gen_crowd_data():\n",
    "    crowd_data = {}\n",
    "    for m in M:\n",
    "        eval_file = crowd_dir+m+'_precessed_results.pickle'\n",
    "        with open(eval_file, 'rb') as f:\n",
    "            processed_results = pickle.load(f)\n",
    "        \n",
    "        sent_list = list(processed_results[\"units\"].index)\n",
    "        print(m)\n",
    "        for i in range(len(sent_list)):\n",
    "            sent_id = sent_list[i]\n",
    "            fss = processed_results[\"units\"][\"unit_annotation_score\"][sent_id]\n",
    "            pred_by_crowd = fss.most_common(1)[0][0]\n",
    "            score = fss.most_common(1)[0][1]\n",
    "            ftuple = (pred_by_crowd, score)\n",
    "            crowd_data[sent_id] = ftuple\n",
    "    return crowd_data\n",
    "            \n",
    "crowd_frame= gen_crowd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crowd_data():\n",
    "    crowd_data = {}\n",
    "    for m in M:\n",
    "        crowd_file = crowd_dir+m+'_result.json'\n",
    "        with open(crowd_file) as f:\n",
    "            crowd = json.load(f)\n",
    "        for sid in crowd:\n",
    "            annotations = crowd[sid]\n",
    "            frame_dict = {}\n",
    "            for anno in annotations:\n",
    "                frame = anno['checked_frame_candidates']\n",
    "                if anno['worker'] != 'kaist_framenet@crowdworks.kr':\n",
    "                    if frame not in frame_dict:\n",
    "                        frame_dict[frame] = anno['aguments']\n",
    "                    else:\n",
    "                        args = frame_dict[frame]\n",
    "                        args += anno['aguments']\n",
    "                        frame_dict[frame] = args\n",
    "            for frame in frame_dict:\n",
    "                all_args = frame_dict[frame]\n",
    "                arg_dict = {}\n",
    "                arg_list = []\n",
    "                for arg in all_args:\n",
    "                    arg_list.append(arg['arg_id'])\n",
    "                arg_list = list(set(arg_list))\n",
    "                for arg_id in arg_list:\n",
    "                    args = []\n",
    "                    for arg in all_args:\n",
    "                        if arg_id == arg['arg_id']:\n",
    "                            args.append(arg['tag'])\n",
    "                    arg_dict[arg_id] = Counter(args)\n",
    "                frame_dict[frame] = arg_dict\n",
    "                \n",
    "            sid = int(sid)\n",
    "            if sid not in crowd_data:\n",
    "                crowd_data[sid] = frame_dict\n",
    "    return crowd_data\n",
    "crowd_data = load_crowd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500\n",
      "[['인도네시아', '환경당국은', '수마트라섬', '중앙부에서', '짙은', '연기가', '급격히', '퍼지고', '있으며', '2', '주전', '우기가', '끝나면서', '증가하기', '시작한', '산불발생', '지역도', '지난달', '28', '일의', '80', '곳에서', '지난', '3', '일에는', '165', '곳으로', '급증했다고', '밝혔다.'], ['_', '_', '_', '_', '_', '_', '_', '퍼지다.v', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['0-0', '0-1', '1-0', '1-1', '2-0', '2-1', '3-0', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']]\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dispersal', 0.617163446529057)\n"
     ]
    }
   ],
   "source": [
    "print(crowd_frame[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dispersal': {0: Counter({'none': 4, 'Individuals': 1, 'Agent': 1}),\n",
      "               1: Counter({'Place': 5, 'Agent': 1}),\n",
      "               2: Counter({'Individuals': 3, 'Cause': 2, 'none': 1}),\n",
      "               3: Counter({'Manner': 5, 'none': 1})},\n",
      " 'Expansion': {0: Counter({'none': 1}),\n",
      "               1: Counter({'Place': 1}),\n",
      "               2: Counter({'Item': 1}),\n",
      "               3: Counter({'Manner': 1})},\n",
      " 'Filling': {0: Counter({'none': 2}),\n",
      "             1: Counter({'Place': 2}),\n",
      "             2: Counter({'Theme': 2}),\n",
      "             3: Counter({'Manner': 2})},\n",
      " 'none': {0: Counter({'': 1}),\n",
      "          1: Counter({'': 1}),\n",
      "          2: Counter({'': 1}),\n",
      "          3: Counter({'': 1})}}\n"
     ]
    }
   ],
   "source": [
    "pprint(crowd_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_kfn11_crowd_data(threshold):\n",
    "    kfn11 = {}\n",
    "    for sid in data:\n",
    "        old_conll = data[sid]\n",
    "        frame, score = crowd_frame[sid]\n",
    "        \n",
    "        if score >= threshold:\n",
    "            if frame != 'none':\n",
    "        \n",
    "                crowd = crowd_data[sid]\n",
    "                tokens = old_conll[0]\n",
    "                lus = old_conll[1]\n",
    "                old_args = old_conll[2]\n",
    "\n",
    "                frames = ['_' for i in range(len(lus))]\n",
    "                for idx in range(len(lus)):\n",
    "                    lu = lus[idx]\n",
    "                    if lu != '_':\n",
    "                        frames[idx] = frame\n",
    "\n",
    "                args = ['O' for i in range(len(lus))]\n",
    "                for argidx in range(len(old_args)):\n",
    "                    old_arg = old_args[argidx]\n",
    "                    if old_arg != '_':\n",
    "                        arg_id = int(old_arg.split('-')[0])\n",
    "                        if old_arg.split('-')[1] == '0':\n",
    "                            bio = 'B-'\n",
    "                        else:\n",
    "                            bio = 'I-'\n",
    "                        if old_arg != '_':\n",
    "                            fe = crowd[frame][arg_id].most_common(1)[0][0]\n",
    "                            if fe != 'none':\n",
    "                                args[argidx] = bio+fe\n",
    "                            \n",
    "                for argidx in range(len(args)):\n",
    "                    if argidx < len(args):\n",
    "                        current_tag = args[argidx]\n",
    "                        next_tag = args[argidx]\n",
    "                        \n",
    "                conll = []\n",
    "                conll.append(tokens)\n",
    "                conll.append(lus)\n",
    "                conll.append(frames)\n",
    "                conll.append(args)\n",
    "                            \n",
    "                if sid not in kfn11:\n",
    "                    kfn11[sid] = conll\n",
    "\n",
    "    if threshold == 0:\n",
    "        fname = '../data/1.1/crowdsourcing/all.json'\n",
    "    elif threshold == 0.5:\n",
    "        fname = '../data/1.1/crowdsourcing/bestF1.json'\n",
    "    elif threshold == 0.9:\n",
    "        fname = '../data/1.1/crowdsourcing/correct.json'\n",
    "    \n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(kfn11, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    return kfn11\n",
    "\n",
    "kfn_a = gen_kfn11_crowd_data(0)\n",
    "kfn_b = gen_kfn11_crowd_data(0.5)\n",
    "kfn_c = gen_kfn11_crowd_data(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kfn10():\n",
    "    with open('../resource/1.0/KFN_annotations.json','r') as f:\n",
    "        kfn10 = json.load(f)\n",
    "    return kfn10\n",
    "kfn10 = load_kfn10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'lu': '이익.n.Earnings_and_losses', 'frame': 'Earnings_and_losses', 'denotations': [{'id': 1, 'obj': 'Earnings_and_losses', 'span': {'begin': 81, 'end': 85}, 'token_span': [18], 'role': 'TARGET', 'text': '순이익이'}, {'id': 2, 'obj': 'Earner', 'token_span': [12, 13, 14, 15, 16], 'span': {'begin': 48, 'end': 76}, 'text': 'Aetna Life and Casualty Co.의', 'role': 'ARGUMENT'}, {'id': 3, 'obj': 'Time', 'token_span': [17], 'span': {'begin': 77, 'end': 80}, 'text': '3분기', 'role': 'ARGUMENT'}], 'relations': [{'subj': 1, 'obj': 3, 'pred': 'arg'}, {'subj': 1, 'obj': 3, 'pred': 'arg'}], 'text': '태풍 Hugo가 남긴 피해들과 회사 내 몇몇 주요 부서들의 저조한 실적들을 반영하여, Aetna Life and Casualty Co.의 3분기 순이익이 182.6 백만 달러 또는 주당 1.63 달러로 22 % 하락하였다.', 'tokens': ['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '순이익이', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], 'sent_id': 'en.FrameNet-1', 'source': 'en.FrameNet'}\n"
     ]
    }
   ],
   "source": [
    "for i in kfn10:\n",
    "    print(i)\n",
    "    print(kfn10[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17434\n",
      "17434\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'begin': 3, 'end': 9}\n"
     ]
    }
   ],
   "source": [
    "def get_token_span(token_ids, text):\n",
    "    text = text.split(' ')\n",
    "    d = {}\n",
    "    cid = -1\n",
    "    for i in range(len(text)):\n",
    "        if i == 0:\n",
    "            d[i] = (0, len(text[i]))\n",
    "            cid = len(text[i]) +1\n",
    "        else:\n",
    "            d[i] = (cid, cid+len(text[i]))\n",
    "            cid =  cid+len(text[i]) +1\n",
    "    start = True\n",
    "    for i in token_ids:\n",
    "        if start == True:\n",
    "            begin = d[i][0]\n",
    "            start = False\n",
    "        end = d[i][1]\n",
    "    span = {}\n",
    "    span['begin'] = begin\n",
    "    span['end'] = end\n",
    "    return span\n",
    "\n",
    "text = '나는 밥을 먹었다'\n",
    "span = get_token_span([1,2], text)\n",
    "print(span)\n",
    "\n",
    "def gen_denotations(tokens, text, frame_index):\n",
    "    # 1) find target\n",
    "    target_ids = []\n",
    "    for token in tokens:\n",
    "        if token[2] != '_':\n",
    "            target_ids.append(int(token[0]))\n",
    "            target = token[2]\n",
    "    span = get_token_span(target_ids, text)\n",
    "    deno = {}\n",
    "    deno['id'] = 1\n",
    "    deno['obj'] = frame_index\n",
    "    deno['span'] = span\n",
    "    deno['token_span'] = target_ids\n",
    "    deno['role'] = 'TARGET'\n",
    "    b = span['begin']\n",
    "    e = span['end']\n",
    "    deno['text'] = text[b:e]\n",
    "    denos = []\n",
    "    denos.append(deno)\n",
    "    # 2) find arguments\n",
    "    fe_id = 2\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i][4] != 'O':\n",
    "            fe_bio = tokens[i][4]\n",
    "            fe_list = fe_bio.split('-')[1:]\n",
    "            fe = '_'.join(fe_list)\n",
    "            if fe_bio.startswith('B'):\n",
    "                d = {}\n",
    "                d['id'] = fe_id\n",
    "                d['obj'] = fe.title()                \n",
    "                n = 1\n",
    "                arg_span = []\n",
    "                arg_span.append(int(tokens[i][0]))\n",
    "                nextfe = False\n",
    "                while i+n < len(tokens):\n",
    "                    nextfe = tokens[i+n][4]\n",
    "                    if nextfe == 'I-'+fe:\n",
    "                        arg_span.append(int(tokens[i+n][0]))\n",
    "                        n += 1\n",
    "                    else:\n",
    "                        break                    \n",
    "                fe_id += 1\n",
    "                d['token_span'] = arg_span\n",
    "                span = get_token_span(arg_span, text)\n",
    "                d['span'] = span\n",
    "                b = span['begin']\n",
    "                e = span['end']\n",
    "                d['text'] = text[b:e]\n",
    "                d['role'] = 'ARGUMENT'\n",
    "                denos.append(d)\n",
    "            else:\n",
    "                pass\n",
    "    if denos:\n",
    "        return denos\n",
    "    else:\n",
    "        print('ERROR_gen_denotations')\n",
    "        \n",
    "def get_relations(denos):\n",
    "    relation = {}\n",
    "    relation['subj'] = 1\n",
    "    relations = []\n",
    "    for i in denos:\n",
    "        if i['role'] == 'ARGUMENT':\n",
    "            did = i['id']\n",
    "            relation['obj'] = did\n",
    "            relation['pred'] = 'arg'\n",
    "            relations.append(relation)\n",
    "    return relations\n",
    "\n",
    "def get_span_lu_frame(conll):\n",
    "    span = []\n",
    "    for idx in range(len(conll)):\n",
    "        i = conll[idx]\n",
    "        if i[2] != '_':\n",
    "            lu = i[2]\n",
    "            frame = i[3]\n",
    "            span.append(idx)\n",
    "    lu = lu+'.'+frame\n",
    "    return span, lu, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lu': '분기.n.Calendric_unit', 'frame': 'Calendric_unit', 'denotations': [{'id': 1, 'obj': 'Calendric_unit', 'span': {'begin': 77, 'end': 80}, 'token_span': [17], 'role': 'TARGET', 'text': '3분기'}, {'id': 2, 'obj': 'Relative_Time', 'token_span': [17], 'span': {'begin': 77, 'end': 80}, 'text': '3분기', 'role': 'ARGUMENT'}], 'relations': [{'subj': 1, 'obj': 2, 'pred': 'arg'}], 'text': '태풍 Hugo가 남긴 피해들과 회사 내 몇몇 주요 부서들의 저조한 실적들을 반영하여, Aetna Life and Casualty Co.의 3분기 순이익이 182.6 백만 달러 또는 주당 1.63 달러로 22 % 하락하였다.', 'tokens': ['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '순이익이', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], 'sent_id': 'en.FrameNet-1', 'source': 'en.FrameNet'}\n"
     ]
    }
   ],
   "source": [
    "h = kfn10['1']\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfn10 17434\n",
      "kfn11 25498\n"
     ]
    }
   ],
   "source": [
    "def gen_kfn11_anno_file():\n",
    "    anno_id = 17437\n",
    "    kfn10 = load_kfn10()\n",
    "    print('kfn10', len(kfn10))\n",
    "    kfn11 = kfn10\n",
    "    for sid in kfn_b:\n",
    "        d = kfn_b[sid]\n",
    "        tokens = d[0]\n",
    "        lus = d[1]\n",
    "        frames = d[2]\n",
    "        args = d[3]     \n",
    "#         for idx in range(len(tokens)):\n",
    "#             if lus[idx] != '_':\n",
    "#                 lu = lus[idx]\n",
    "#                 frame = frames[idx]\n",
    "        text = ' '.join(tokens)\n",
    "        sent_id = 'crowdsourcing-'+str(sid)\n",
    "        source = sid2source(int(sid))\n",
    "        \n",
    "        tokids = []\n",
    "        for tokid in range(len(tokens)):\n",
    "            tokids.append(str(tokid))\n",
    "            \n",
    "        conll = []\n",
    "        for tokid in range(len(tokens)):\n",
    "            t = []\n",
    "            t.append(tokids[tokid])\n",
    "            t.append(tokens[tokid])\n",
    "            t.append(lus[tokid])\n",
    "            t.append(frames[tokid])\n",
    "            t.append(args[tokid])\n",
    "            conll.append(t)\n",
    "        \n",
    "        tgt_span, lu, frame = get_span_lu_frame(conll)\n",
    "        denos = gen_denotations(conll, text, frame)\n",
    "        relations = get_relations(denos)\n",
    "        \n",
    "        item = {}\n",
    "        item['lu'] = lu\n",
    "        item['frame'] = frame\n",
    "        item['denotations'] = denos\n",
    "        item['relations'] = relations\n",
    "        item['text'] = text\n",
    "        item['tokens'] = tokens\n",
    "        item['sent_id'] = sent_id\n",
    "        item['source'] = source\n",
    "        \n",
    "        annotation_id = str(anno_id)\n",
    "        anno_id +=1\n",
    "        if annotation_id not in kfn11:\n",
    "            kfn11[annotation_id] = item\n",
    "        else:\n",
    "            print(annotation_id)\n",
    "            \n",
    "    print('kfn11',len(kfn11))\n",
    "    \n",
    "    \n",
    "    return kfn11\n",
    "    \n",
    "kfn11 = gen_kfn11_anno_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_kfn11 25483\n",
      "written is done\n"
     ]
    }
   ],
   "source": [
    "old_kfn11 = kfn11\n",
    "new_kfn11 = {}\n",
    "\n",
    "for i in old_kfn11:\n",
    "    item = old_kfn11[i]\n",
    "    \n",
    "    ok = ['육개.n.Cardinal_numbers', '년.n.Cardinal_numbers', '모두.n.Cardinal_numbers', '사백.n.Cardinal_numbers', '삼백.n.Cardinal_numbers', '영하.n.Cardinal_numbers', '육십.n.Cardinal_numbers', '이틀.n.Cardinal_numbers', '일.n.Cardinal_numbers', '일련.n.Cardinal_numbers', '하루.n.Cardinal_numbers']\n",
    "    \n",
    "    if item['frame'] == 'Cardinal_numbers':\n",
    "        if item['lu'] in ok:\n",
    "            new_kfn11[i] = item\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    else:\n",
    "        new_kfn11[i] = item\n",
    "        \n",
    "print('new_kfn11', len(new_kfn11))\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "with open('../resource/1.1/KFN_annotations.json','w') as f:\n",
    "    json.dump(new_kfn11, f, ensure_ascii=False, indent=4)    \n",
    "print('written is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfn11 25483\n",
      "kfn11 25483\n"
     ]
    }
   ],
   "source": [
    "def kfn2data():\n",
    "    with open('../resource/1.1/KFN_annotations.json','r') as f:\n",
    "        kfn11 = json.load(f)\n",
    "        \n",
    "    print('kfn11', len(kfn11))\n",
    "       \n",
    "    \n",
    "    kfn11_data = {}\n",
    "    for anno_id in kfn11:\n",
    "\n",
    "        lemma = kfn11[anno_id]['lu'].split('.')[0]\n",
    "        pos = kfn11[anno_id]['lu'].split('.')[1]\n",
    "        \n",
    "        lu = lemma+'.'+pos\n",
    "        frame = kfn11[anno_id]['frame']\n",
    "        tokens = kfn11[anno_id]['tokens']\n",
    "        lus = ['_' for i in range(len(tokens))]\n",
    "        frames = ['_' for i in range(len(tokens))]\n",
    "        args = ['O' for i in range(len(tokens))]\n",
    "        for deno in kfn11[anno_id]['denotations']:\n",
    "            if deno['role'] == 'TARGET':\n",
    "                for tokid in deno['token_span']:\n",
    "                    lus[tokid] = lu\n",
    "                    frames[tokid] = frame\n",
    "        for deno in kfn11[anno_id]['denotations']:\n",
    "            if deno['role'] == 'ARGUMENT':\n",
    "                for aidx in range(len(deno['token_span'])):\n",
    "                    tokid = deno['token_span'][aidx]\n",
    "                    if aidx == 0:\n",
    "                        bio = 'B-'\n",
    "                    else:\n",
    "                        bio = 'I-'\n",
    "                    fe = bio+deno['obj']\n",
    "                    args[tokid] = fe\n",
    "        tokid = 0\n",
    "        tokids = []\n",
    "        for i in range(len(tokens)):\n",
    "            tokids.append(str(tokid))\n",
    "            tokid +=1\n",
    "        \n",
    "        conll = []\n",
    "        for tokid in range(len(tokens)):\n",
    "            t = []\n",
    "            t.append(tokids[tokid])\n",
    "            t.append(tokens[tokid])\n",
    "            t.append(lus[tokid])\n",
    "            t.append(frames[tokid])\n",
    "            t.append(args[tokid])\n",
    "            conll.append(t)\n",
    "        \n",
    "        kfn11_data[anno_id] = conll\n",
    "        \n",
    "    print('kfn11', len(kfn11_data))\n",
    "        \n",
    "    return kfn11_data\n",
    "kfn11_data = kfn2data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 25483\n",
      "tst 5097\n",
      "dev 2548\n",
      "trn 17838\n",
      "tst 5097\n",
      "dev 2548\n",
      "trn 17838\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "all_sents = list(kfn11_data.keys())\n",
    "\n",
    "tst = random.sample(all_sents, 5097)\n",
    "sents = [i for i in all_sents if not i in tst]\n",
    "dev = random.sample(sents, 2548)\n",
    "trn = [i for i in sents if not i in dev]\n",
    "\n",
    "\n",
    "print('all', len(all_sents))\n",
    "print('tst',len(tst))\n",
    "print('dev', len(dev))\n",
    "print('trn', len(trn))\n",
    "\n",
    "\n",
    "tst_data, dev_data, trn_data = {},{},{}\n",
    "\n",
    "for i in kfn11_data:\n",
    "    item = kfn11_data[i]\n",
    "    if i in tst:\n",
    "        tst_data[i] = item\n",
    "    elif i in dev:\n",
    "        dev_data[i] = item\n",
    "    elif i in trn:\n",
    "        trn_data[i] = item\n",
    "        \n",
    "print('tst',len(tst_data))\n",
    "print('dev', len(dev_data))\n",
    "print('trn', len(trn_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/1.1/training.tsv','w') as f:\n",
    "    for i in trn_data:\n",
    "        item = trn_data[i]\n",
    "        \n",
    "        text_list = []\n",
    "        for t in item:\n",
    "            text_list.append(t[1])\n",
    "        text = ' '.join(text_list)\n",
    "        \n",
    "        line = '#anno-id:'+str(i)+'\\n'\n",
    "        f.write(line)\n",
    "        \n",
    "        line = '#text:'+text+'\\n'\n",
    "        f.write(line)\n",
    "        \n",
    "        for i in item:\n",
    "            line = '\\t'.join(i)+'\\n'\n",
    "            f.write(line)\n",
    "        \n",
    "        line = '\\n'\n",
    "        f.write(line)\n",
    "        \n",
    "with open('../data/1.1/test.tsv','w') as f:\n",
    "    for i in tst_data:\n",
    "        item = tst_data[i]\n",
    "        \n",
    "        text_list = []\n",
    "        for t in item:\n",
    "            text_list.append(t[1])\n",
    "        text = ' '.join(text_list)\n",
    "        \n",
    "        line = '#anno-id:'+str(i)+'\\n'\n",
    "        f.write(line)\n",
    "        \n",
    "        line = '#text:'+text+'\\n'\n",
    "        f.write(line)\n",
    "        \n",
    "        for i in item:\n",
    "            line = '\\t'.join(i)+'\\n'\n",
    "            f.write(line)\n",
    "        \n",
    "        line = '\\n'\n",
    "        f.write(line)\n",
    "        \n",
    "with open('../data/1.1/dev.tsv','w') as f:\n",
    "    for i in dev_data:\n",
    "        item = dev_data[i]\n",
    "        \n",
    "        text_list = []\n",
    "        for t in item:\n",
    "            text_list.append(t[1])\n",
    "        text = ' '.join(text_list)\n",
    "        \n",
    "        line = '#anno-id:'+str(i)+'\\n'\n",
    "        f.write(line)\n",
    "        \n",
    "        line = '#text:'+text+'\\n'\n",
    "        f.write(line)\n",
    "        \n",
    "        for i in item:\n",
    "            line = '\\t'.join(i)+'\\n'\n",
    "            f.write(line)\n",
    "        \n",
    "        line = '\\n'\n",
    "        f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
